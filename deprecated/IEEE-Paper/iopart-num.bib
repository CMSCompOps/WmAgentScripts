@techreport{Aderholz2000,
      author       = "Aderholz, Michael and Amako, K and Augé, E and Bagliesi, G
                      and Barone, L and Battistoni, G and Bernardi, M and
                      Boschini, M and Brunengo, A and Bunn, J J and Butler, J and
                      Campanella, M and Capiluppi, P and Carminati, F and D'Amato,
                      M and Dameri, M and Di Mattia, A and Dorokhov, A E and
                      Erbacci, G and Gasparini, U and Gagliardi, F and Gaines, I
                      and Gálvez, P and Ghiselli, A and Gordon, J and Grandi, C
                      and Harris, F and Holtman, K and Karimäki, V and Karita, Y
                      and Klem, J T and Legrand, I and Leltchouk, M and Linglin, D
                      and Lubrano, P and Luminari, L and Maslennikov, A L and
                      Mattasoglio, A and Michelotto, M and McArthur, I C and
                      Morita, Y and Nazarenko, A and Newman, H and O'Dell, Vivian
                      and O'Neale, S W and Osculati, B and Pepé, M and Perini, L
                      and Pinfold, James L and Pordes, R and Prelz, F and Putzer,
                      A and Resconi, S and Robertson, L and Rolli, S and Sasaki, T
                      and Sato, H and Servoli, L and Schaffer, R D and Schalk, T L
                      and Sgaravatto, M and Shiers, J and Silvestris, L and
                      Siroli, G P and Sliwa, K and Smith, T and Somigliana, R and
                      Stanescu, C and Stockinger, H E and Ugolotti, D and Valente,
                      E and Vistoli, C and Willers, Ian Malcolm and Wilkinson, R P
                      and Williams, D O",
      title        = "Models Of Networked Analysis At Regional Centres For Lhc
                      Experiments (monarc), Phase 2 Report, 24th March 2000",
      institution  = "CERN",
      address      = "Geneva",
      number       = "CERN-LCB-2000-001. KEK-2000-8",
      month        = "Apr",
      year         = "2000",
}


@article{GGUS2010,
  author={T Antoni and D Bosio and M Dimou},
  title={WLCG-specific special features in GGUS},
  journal={Journal of Physics: Conference Series},
  volume={219},
  number={6},
  pages={062032},
  url={http://stacks.iop.org/1742-6596/219/i=6/a=062032},
  year={2010},
  abstract={The user and operations support of the EGEE series of projects can be captioned "regional support with central coordination". Its central building block is the GGUS portal which acts as an entry point for users and support staff. It is also as an integration platform for the distributed support effort. As WLCG relies heavily on the EGEE infrastructure it is important that the support infrastructure covers the WLCG use cases of the grid. During the last year several special features have been implemented in the GGUS portal to meet the requirements of the LHC experiments needing to contact the WLCG grid infrastructure, especially their Tier 1 and Tier 2 centres. This paper summarises these special features, with particular focus on the alarm and team tickets and the direct ticket routing, in the context of the overall user and operations support infrastructure. Additionally we will present the management processes for the user support activity, detailing the options which the LHC VOs have to participate in this process. An outlook will be given on how the user support activity will evolve towards the EGI/NGI model without disrupting the production quality service provided by EGEE for WLCG.}
}
	


@article{Julia2011,
  author={J Andreeva and M Devesas Campos and J Tarragon Cros and B Gaidioz and E Karavakis and L Kokoszkiewicz and E Lanciotti and G
Maier and W Ollivier and M Nowotka and R Rocha and T Sadykov and P Saiz and L Sargsyan and I Sidorova and D Tuckett},
  title={Experiment Dashboard for Monitoring of the LHC Distributed Computing Systems},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072001},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072001},
  year={2011},
  abstract={LHC experiments are currently taking collisions data. A distributed computing model chosen by the four main LHC experiments allows physicists to benefit from resources spread all over the world. The distributed model and the scale of LHC computing activities increase the level of complexity of middleware, and also the chances of possible failures or inefficiencies in involved components. In order to ensure the required performance and functionality of the LHC computing system, monitoring the status of the distributed sites and services as well as monitoring LHC computing activities are among the key factors. Over the last years, the Experiment Dashboard team has been working on a number of applications that facilitate the monitoring of different activities: including following up jobs, transfers, and also site and service availabilities. This presentation describes Experiment Dashboard applications used by the LHC experiments and experience gained during the first months of data taking.}
}



@article{DBS2008,
  author={A Afaq and A Dolgert and Y Guo and C Jones and S Kosyakov and V Kuznetsov and L Lueking and D Riley and V Sekhri},
  title={The CMS dataset bookkeeping service},
  journal={Journal of Physics: Conference Series},
  volume={119},
  number={7},
  pages={072001},
  url={http://stacks.iop.org/1742-6596/119/i=7/a=072001},
  year={2008},
  abstract={The CMS Dataset Bookkeeping Service (DBS) has been developed to catalog all CMS event data from Monte Carlo and Detector sources. It provides the ability to identify MC or trigger source, track data provenance, construct datasets for analysis, and discover interesting data. CMS requires processing and analysis activities at various service levels and the DBS system provides support for localized processing or private analysis, as well as global access for CMS users at large. Catalog entries can be moved among the various service levels with a simple set of migration tools, thus forming a loose federation of databases. DBS is available to CMS users via a Python API, Command Line, and a Discovery web page interfaces. The system is built as a multi-tier web application with Java servlets running under Tomcat, with connections via JDBC to Oracle or MySQL database backends. Clients connect to the service through HTTP or HTTPS with authentication provided by GRID certificates and authorization through VOMS. DBS is an integral part of the overall CMS Data Management and Workflow Management systems.}
}
	


@article{Nicolo2011,
  author={Nicolo Magini and Natalia Ratnikova and Paul Rossman and Alberto S\'anchez-Hern\'andez and Tony Wildish},
  title={Distributed data transfers in CMS},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={4},
  pages={042036},
  url={http://stacks.iop.org/1742-6596/331/i=4/a=042036},
  year={2011},
  abstract={The multi-tiered computing infrastructure of the CMS experiment at the LHC depends on the reliable and fast transfer of data between the different CMS computing sites. Data have to be transferred from the Tier-0 to the Tier-l sites for archival in a timely manner to avoid overflowing disk buffers at CERN. Data have to be transferred in bursts to all Tier-2 level sites for analysis as well as synchronized between the different Tier-l sites. The data transfer system is the key ingredient which enables the optimal usage of all distributed resources. The operation of the transfer system consists of monitoring and debugging of transfer issues to guarantee a timely delivery of data to all corners of the CMS computing infrastructure. Further task of transfer operation is to guarantee the consistency of the data at all sites, both on disk and on tape. Procedures to verify the consistency and to debug and repair problems will be discussed.}
}
	



@article{Zdenek2012,
title={Alert Messaging in the CMS Distributed Workload System},
journal={Journal of Physics: Conference Series},
note={CHEP 2012},
author={Zdenek Maxa},
year={2012}
}

@article{Rapolas2012,
title={CMS Data Transfer operations after the first years of LHC collisions},
journal={Journal of Physics: Conference Series},
note={CHEP 2012},
author={Rapolas Kaselis and Nicolo Magini and Andrea Sartirana and Peter Kreuzer and Jose Flix and Oliver Gutsche and Markus Klute and Stefan Piperov},
year={2012}
}




@article{Igor2012,
title={The benefits and challenges of sharing glidein factory operations across nine time zones between OSG and CMS},
journal={Journal of Physics: Conference Series},
note={CHEP 2012},
author={Igor Sfiligoi and Jeff Dost and Marian Zvada and Ignas Butenas and Frank Wuerthwein and Peter Kreuzer and Scott Teige and Rob Quick and Jose Hern\'andez and Jose Flix},
year={2012}
}




@article{Igor2009,
author = {Igor Sfiligoi and Daniel C. Bradley and Burt Holzman and Parag Mhashilkar and Sanjay Padhi and Frank Wurthwein},
title = {The Pilot Way to Grid Resources Using glideinWMS},
journal ={Computer Science and Information Engineering, World Congress on},
volume = {2},
isbn = {978-0-7695-3507-4},
year = {2009},
pages = {428-432},
doi = {http://doi.ieeecomputersociety.org/10.1109/CSIE.2009.950},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}
  	
@article{Bonacorsi2011,
  author={Daniele Bonacorsi and the CMS Computing project},
  title={Experience with the CMS Computing Model from commissioning to collisions},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072005},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072005},
  year={2011},
  abstract={In this presentation we will discuss the early experience with the CMS computing model from the last large scale challenge activities through the first six months of data taking. Between the initial definition of the CMS Computing Model in 2004 and the start of high energy collisions in 2010, CMS exercised the infrastructure with numerous scaling tests and service challenges. We will discuss how those tests have helped prepare the experiment for operations and how representative the challenges were to the early experience with data taking. We will outline how the experiment operations has evolved during the first few months of operations. The current state of the Computing system will be presented and we will describe the initial experience with active users and real data. We will address the issues that worked well in addition to identifying areas where future development and refinement is needed.}
}
	
@ONLINE{elog,
author = {Workflow Team},
title = {Workflow team e-log},
month = jun,
year = {2012},
url = {https://cmslogbook.cern.ch/elog/Workflow+processing/}
}


@article{Jen2010,
  author={Jennifer Adelman-McCarthy and Oliver Gutsche and Jeffrey D Haas and Harrison B Prosper and Valentina Dutta and Guillelmo
Gomez-Ceballos and Kristian Hahn and Markus Klute and Ajit Mohapatra and Vincenzo Spinoso and Dorian Kcira and Julien
Caudron and Junhui Liao and Arnaud Pin and Nicolas Schul and Gilles De Lentdecker and Joseph McCartin and Lukas
Vanelderen and Xavier Janssen and Andrey Tsyganov and Derek Barge and Andrew Lahiff},
  title={CMS distributed computing workflow experience},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072019},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072019},
  year={2011},
  abstract={The vast majority of the CMS Computing capacity, which is organized in a tiered hierarchy, is located away from CERN. The 7 Tier-1 sites archive the LHC proton-proton collision data that is initially processed at CERN. These sites provide access to all recorded and simulated data for the Tier-2 sites, via wide-area network (WAN) transfers. All central data processing workflows are executed at the Tier-1 level, which contain re-reconstruction and skimming workflows of collision data as well as reprocessing of simulated data to adapt to changing detector conditions. This paper describes the operation of the CMS processing infrastructure at the Tier-1 level. The Tier-1 workflows are described in detail. The operational optimization of resource usage is described. In particular, the variation of different workflows during the data taking period of 2010, their efficiencies and latencies as well as their impact on the delivery of physics results is discussed and lessons are drawn from this experience. The simulation of proton-proton collisions for the CMS experiment is primarily carried out at the second tier of the CMS computing infrastructure. Half of the Tier-2 sites of CMS are reserved for central Monte Carlo (MC) production while the other half is available for user analysis. This paper summarizes the large throughput of the MC production operation during the data taking period of 2010 and discusses the latencies and efficiencies of the various types of MC production workflows. We present the operational procedures to optimize the usage of available resources and we the operational model of CMS for including opportunistic resources, such as the larger Tier-3 sites, into the central production operation.}
}

@article{Evans2008,
title = "The CMS Monte Carlo Production System: Development and Design",
journal = "Nuclear Physics B - Proceedings Supplements",
volume = "177-178",
number = "0",
pages = "285 - 286",
year = "2008",
note = "Proceedings of the Hadron Collider Physics Symposium 2007",
issn = "0920-5632",
doi = "10.1016/j.nuclphysbps.2008.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0920563208000418",
author = "D. Evans and A. Fanfani and C. Kavka and F. van Lingen and G. Eulisse and W. Bacchi and G. Codispoti and D. Mason and N. De Filippis and J.M. Hern\'andez and P. Elmer"
}

@article{Wakefield2012,
title={The WorkQueue project - a task queue for the CMS workload management system},
journal={Journal of Physics: Conference Series},
note={CHEP 2012},
author={Stuart Wakefield and Seangchan Ryu and Dave Evans and Simon Metson and Stephen Foulkes and Matthew Norman and Zdenek Maxa},
year={2012}
}



